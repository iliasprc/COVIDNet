{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVIDNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TJJOOlgfUS5",
        "colab_type": "text"
      },
      "source": [
        "# DATASET EXTRACTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZM9zRc_hfZ",
        "colab_type": "text"
      },
      "source": [
        "## clone githubs and unzip the two datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG5w1SXpEutA",
        "colab_type": "code",
        "outputId": "ca005b32-4168-4ede-84e7-29bdf37c9d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "! git clone https://github.com/ieee8023/covid-chestxray-dataset.git\n",
        "! git clone https://github.com/IliasPap/COVIDNet.git\n",
        "\n",
        "COPY_FILE = True\n",
        "\n",
        "# # !pip install pydicom\n",
        "# ! pip install -q kaggle\n",
        "# ! mkdir ~/.kaggle\n",
        "\n",
        "# ! pip install kaggle==1.5.6\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "! mkdir /content/rsna_dataset\n",
        "! unzip -q '/content/drive/My Drive/MEDICAL/rsna-pneumonia-detection-challenge.zip' -d /content/rsna_dataset/\n",
        "\n",
        "# ! kaggle competitions download -c rsna-pneumonia-detection-challenge"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'covid-chestxray-dataset'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 1606 (delta 40), reused 66 (delta 23), pack-reused 1511\u001b[K\n",
            "Receiving objects: 100% (1606/1606), 219.51 MiB | 1.17 MiB/s, done.\n",
            "Resolving deltas: 100% (764/764), done.\n",
            "Checking out files: 100% (350/350), done.\n",
            "Cloning into 'COVIDNet'...\n",
            "remote: Enumerating objects: 156, done.\u001b[K\n",
            "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 156 (delta 67), reused 83 (delta 23), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (156/156), 47.95 MiB | 7.28 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "unzip:  cannot find or open /content/drive/My Drive/MEDICAL/rsna-pneumonia-detection-challenge.zip, /content/drive/My Drive/MEDICAL/rsna-pneumonia-detection-challenge.zip.zip or /content/drive/My Drive/MEDICAL/rsna-pneumonia-detection-challenge.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL4jqTfNp59A",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5wJrNmlhvjT",
        "colab_type": "code",
        "outputId": "1a16dbe0-e942-45f6-b89c-ab835accf802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "! pip install pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random \n",
        "from shutil import copyfile\n",
        "import pydicom as dicom\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/e6/4cae2b4b2fdbea5e2ddd188361139606d8f10f710ba1abecd6600da099c3/pydicom-1.4.2-py2.py3-none-any.whl (35.3MB)\n",
            "\u001b[K     |████████████████████████████████| 35.3MB 96kB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWIYPPL9h1xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "seed = 0\n",
        "np.random.seed(seed) # Reset the seed so all runs are the same.\n",
        "random.seed(seed)\n",
        "MAXVAL = 255  # Range [0 255]\n",
        "root = '/content/covid-chestxray-dataset'\n",
        "\n",
        "if (COPY_FILE):\n",
        "    savepath = root + '/data'\n",
        "    if(not os.path.exists(savepath)):\n",
        "        os.makedirs(savepath)\n",
        "    savepath = root + '/data/train'\n",
        "    if(not os.path.exists(savepath)):\n",
        "        os.makedirs(savepath)\n",
        "    savepath = root + '/data/test'\n",
        "    if(not os.path.exists(savepath)):\n",
        "        os.makedirs(savepath)\n",
        "\n",
        "savepath = root + '/data'\n",
        "# path to covid-19 dataset from https://github.com/ieee8023/covid-chestxray-dataset\n",
        "imgpath = root + '/images' \n",
        "csvpath = root + '/metadata.csv'\n",
        "\n",
        "# path to https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\n",
        "kaggle_datapath = '/content/rsna_kaggle_dataset'\n",
        "kaggle_csvname = 'stage_2_detailed_class_info.csv' # get all the normal from here\n",
        "kaggle_csvname2 = 'stage_2_train_labels.csv' # get all the 1s from here since 1 indicate pneumonia\n",
        "kaggle_imgpath = 'stage_2_train_images'\n",
        "\n",
        "# parameters for COVIDx dataset\n",
        "train = []\n",
        "test = []\n",
        "test_count = {'normal': 0, 'pneumonia': 0, 'COVID-19': 0}\n",
        "train_count = {'normal': 0, 'pneumonia': 0, 'COVID-19': 0}\n",
        "\n",
        "mapping = dict()\n",
        "mapping['COVID-19'] = 'COVID-19'\n",
        "mapping['SARS'] = 'pneumonia'\n",
        "mapping['MERS'] = 'pneumonia'\n",
        "mapping['Streptococcus'] = 'pneumonia'\n",
        "mapping['Normal'] = 'normal'\n",
        "mapping['Lung Opacity'] = 'pneumonia'\n",
        "mapping['1'] = 'pneumonia'\n",
        "\n",
        "# train/test split\n",
        "split = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A40xmYriH6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adapted from https://github.com/mlmed/torchxrayvision/blob/master/torchxrayvision/datasets.py#L814\n",
        "csv = pd.read_csv(csvpath, nrows=None)\n",
        "idx_pa = csv[\"view\"] == \"PA\"  # Keep only the PA view\n",
        "csv = csv[idx_pa]\n",
        "\n",
        "pneumonias = [\"COVID-19\", \"SARS\", \"MERS\", \"ARDS\", \"Streptococcus\"]\n",
        "pathologies = [\"Pneumonia\",\"Viral Pneumonia\", \"Bacterial Pneumonia\", \"No Finding\"] + pneumonias\n",
        "pathologies = sorted(pathologies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oCGULcBibv-",
        "colab_type": "code",
        "outputId": "1c6e13e5-c06b-4598-95bc-98c7cd121e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# get non-COVID19 viral, bacteria, and COVID-19 infections from covid-chestxray-dataset\n",
        "# stored as patient id, image filename and label\n",
        "filename_label = {'normal': [], 'pneumonia': [], 'COVID-19': []}\n",
        "count = {'normal': 0, 'pneumonia': 0, 'COVID-19': 0}\n",
        "print(csv.keys())\n",
        "for index, row in csv.iterrows():\n",
        "    f = row['finding']\n",
        "    if f in mapping:\n",
        "        count[mapping[f]] += 1\n",
        "        entry = [int(row['patientid']), row['filename'], mapping[f]]\n",
        "        filename_label[mapping[f]].append(entry)\n",
        "\n",
        "print('Data distribution from covid-chestxray-dataset:')\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['patientid', 'offset', 'sex', 'age', 'finding', 'survival',\n",
            "       'temperature', 'pO2 saturation', 'leukocyte count', 'neutrophil count',\n",
            "       'lymphocyte count', 'view', 'modality', 'date', 'location', 'folder',\n",
            "       'filename', 'doi', ' url', 'license', 'clinical notes', 'other notes',\n",
            "       'Unnamed: 22'],\n",
            "      dtype='object')\n",
            "Data distribution from covid-chestxray-dataset:\n",
            "{'normal': 0, 'pneumonia': 24, 'COVID-19': 99}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZXOLpHZiz3q",
        "colab_type": "code",
        "outputId": "a04c76eb-f616-4d26-8fa3-e651a2fc8384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# add covid-chestxray-dataset into COVIDx dataset\n",
        "# since covid-chestxray-dataset doesn't have test dataset\n",
        "# split into train/test by patientid\n",
        "# for COVIDx:\n",
        "# patient 8 is used as non-COVID19 viral test\n",
        "# patient 31 is used as bacterial test\n",
        "# patients 19, 20, 36, 42, 86 are used as COVID-19 viral test\n",
        "\n",
        "for key in filename_label.keys():\n",
        "    arr = np.array(filename_label[key])\n",
        "    if arr.size == 0:\n",
        "        continue\n",
        "    # split by patients\n",
        "    # num_diff_patients = len(np.unique(arr[:,0]))\n",
        "    # num_test = max(1, round(split*num_diff_patients))\n",
        "    # select num_test number of random patients\n",
        "    if key == 'pneumonia':\n",
        "        test_patients = ['8', '31']\n",
        "    elif key == 'COVID-19':\n",
        "        test_patients = ['19', '20', '36', '42', '86'] # random.sample(list(arr[:,0]), num_test)\n",
        "    else: \n",
        "        test_patients = []\n",
        "    print('Key: ', key)\n",
        "    print('Test patients: ', test_patients)\n",
        "    # go through all the patients\n",
        "    for patient in arr:\n",
        "        if patient[0] in test_patients:\n",
        "            if (COPY_FILE):\n",
        "                copyfile(os.path.join(imgpath, patient[1]), os.path.join(savepath, 'test', patient[1]))\n",
        "                test.append(patient)\n",
        "                test_count[patient[2]] += 1\n",
        "            else:\n",
        "                print(\"WARNING   :   passing copy file !!!!!!!!!!!!!!!!!!!!!!\")\n",
        "                break\n",
        "        else:\n",
        "            if (COPY_FILE):\n",
        "                copyfile(os.path.join(imgpath, patient[1]), os.path.join(savepath, 'train', patient[1]))\n",
        "                train.append(patient)\n",
        "                train_count[patient[2]] += 1\n",
        "\n",
        "            else:\n",
        "                print(\"WARNING   :   passing copy file !!!!!!!!!!!!!!!!!!!!!!\")\n",
        "                break\n",
        "\n",
        "print('test count: ', test_count)\n",
        "print('train count: ', train_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key:  pneumonia\n",
            "Test patients:  ['8', '31']\n",
            "Key:  COVID-19\n",
            "Test patients:  ['19', '20', '36', '42', '86']\n",
            "test count:  {'normal': 0, 'pneumonia': 5, 'COVID-19': 10}\n",
            "train count:  {'normal': 0, 'pneumonia': 19, 'COVID-19': 89}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW2HHFi1Dra2",
        "colab_type": "text"
      },
      "source": [
        "## COPY KAGGLE DATA TO TRAIN AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG-qk6gXjZIB",
        "colab_type": "code",
        "outputId": "172e7e65-cdb8-4ffe-d12f-844d0302a1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# add normal and rest of pneumonia cases from https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\n",
        "\n",
        "\n",
        "kaggle_datapath = '/content/rsna_dataset'\n",
        "\n",
        "print(kaggle_datapath)\n",
        "csv_normal = pd.read_csv(os.path.join(kaggle_datapath, kaggle_csvname), nrows=None)\n",
        "csv_pneu = pd.read_csv(os.path.join(kaggle_datapath, kaggle_csvname2), nrows=None)\n",
        "patients = {'normal': [], 'pneumonia': []}\n",
        "\n",
        "for index, row in csv_normal.iterrows():\n",
        "    if row['class'] == 'Normal':\n",
        "        patients['normal'].append(row['patientId'])\n",
        "\n",
        "for index, row in csv_pneu.iterrows():\n",
        "    if int(row['Target']) == 1:\n",
        "        patients['pneumonia'].append(row['patientId'])\n",
        "\n",
        "for key in patients.keys():\n",
        "    arr = np.array(patients[key])\n",
        "    if arr.size == 0:\n",
        "        continue\n",
        "    # split by patients \n",
        "    # num_diff_patients = len(np.unique(arr))\n",
        "    # num_test = max(1, round(split*num_diff_patients))\n",
        "    #'/content/COVID-Net/'\n",
        "    test_patients = np.load('/content/COVID-Net/rsna_test_patients_{}.npy'.format(key)) # random.sample(list(arr), num_test)\n",
        "    # np.save('rsna_test_patients_{}.npy'.format(key), np.array(test_patients))\n",
        "    for patient in arr:\n",
        "        ds = dicom.dcmread(os.path.join(kaggle_datapath, kaggle_imgpath, patient + '.dcm'))\n",
        "        pixel_array_numpy = ds.pixel_array\n",
        "        imgname = patient + '.png'\n",
        "        if patient in test_patients:\n",
        "            if (COPY_FILE):\n",
        "                cv2.imwrite(os.path.join(savepath, 'test', imgname), pixel_array_numpy)\n",
        "                test.append([patient, imgname, key])\n",
        "                test_count[key] += 1\n",
        "            else:\n",
        "                print(\"WARNING   :   passing copy file !!!!!!!!!!!!!!!!!!!!!!\")\n",
        "                break\n",
        "        else:\n",
        "            if (COPY_FILE):\n",
        "                cv2.imwrite(os.path.join(savepath, 'train', imgname), pixel_array_numpy)\n",
        "                train.append([patient, imgname, key])\n",
        "                train_count[key] += 1\n",
        "            else:\n",
        "                print(\"WARNING   :   passing copy file !!!!!!!!!!!!!!!!!!!!!!\")\n",
        "                break\n",
        "print('test count: ', test_count)\n",
        "print('train count: ', train_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/rsna_dataset\n",
            "test count:  {'normal': 885, 'pneumonia': 1058, 'COVID-19': 10}\n",
            "train count:  {'normal': 7966, 'pneumonia': 8521, 'COVID-19': 89}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ4gYQkDxt_",
        "colab_type": "text"
      },
      "source": [
        "## Final data stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rofg8ddlX28e",
        "colab_type": "code",
        "outputId": "a0a6f9e6-0f7b-43ad-8d7a-9a6b9d381dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# final stats\n",
        "print('Final stats')\n",
        "print('Train count: ', train_count)\n",
        "print('Test count: ', test_count)\n",
        "print('Total length of train: ', len(train))\n",
        "print('Total length of test: ', len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final stats\n",
            "Train count:  {'normal': 7966, 'pneumonia': 8521, 'COVID-19': 89}\n",
            "Test count:  {'normal': 885, 'pneumonia': 1058, 'COVID-19': 10}\n",
            "Total length of train:  16576\n",
            "Total length of test:  1953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zruYtB-8D1y4",
        "colab_type": "text"
      },
      "source": [
        "## train and test file extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh_E3s-AX3sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export to train and test csv\n",
        "# format as patientid, filename, label, separated by a space\n",
        "train_file = open(\"train_split_v2.txt\",\"w\") \n",
        "for sample in train:\n",
        "    info = str(sample[0]) + ' ' + sample[1] + ' ' + sample[2] + '\\n'\n",
        "    train_file.write(info)\n",
        "\n",
        "train_file.close()\n",
        "\n",
        "test_file = open(\"test_split_v2.txt\", \"w\")\n",
        "for sample in test:\n",
        "    info = str(sample[0]) + ' ' + sample[1] + ' ' + sample[2] + '\\n'\n",
        "    test_file.write(info)\n",
        "\n",
        "test_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuSxkmuebDuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import glob\n",
        "\n",
        "# images = glob.glob('/content/drive/My Drive/MEDICAL/data/*/*')\n",
        "# print(len(images))\n",
        "\n",
        "# train = glob.glob('/content/drive/My Drive/MEDICAL/data/train/*')\n",
        "# test = glob.glob('/content/drive/My Drive/MEDICAL/data/test/*')\n",
        "# print(len(train))\n",
        "# print(len(test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJqeEfwh0CH1",
        "colab_type": "text"
      },
      "source": [
        "# Training imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvCMyZ_m7LN7",
        "colab_type": "code",
        "outputId": "c7377e8f-f6e6-4610-9e1a-abe46762c189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "! pip install torchsummaryX\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading https://files.pythonhosted.org/packages/36/23/87eeaaf70daa61aa21495ece0969c50c446b8fd42c4b8905af264b40fe7f/torchsummaryX-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.18.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->torchsummaryX) (1.12.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bef7LpK60EsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from torchsummaryX import summary\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pu8yV5AN1i",
        "colab_type": "text"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S1MSMhTAPkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def write_score(writer, iter, mode, metrics):\n",
        "    writer.add_scalar(mode + '/loss', metrics.data['loss'], iter)\n",
        "    writer.add_scalar(mode + '/acc', metrics.data['correct'] / metrics.data['total'], iter)\n",
        "\n",
        "\n",
        "def write_train_val_score(writer, epoch, train_stats, val_stats):\n",
        "    writer.add_scalars('Loss', {'train': train_stats[0],\n",
        "                                'val': val_stats[0],\n",
        "                                }, epoch)\n",
        "    writer.add_scalars('Coeff', {'train': train_stats[1],\n",
        "                                 'val': val_stats[1],\n",
        "                                 }, epoch)\n",
        "\n",
        "    writer.add_scalars('Air', {'train': train_stats[2],\n",
        "                               'val': val_stats[2],\n",
        "                               }, epoch)\n",
        "\n",
        "    writer.add_scalars('CSF', {'train': train_stats[3],\n",
        "                               'val': val_stats[3],\n",
        "                               }, epoch)\n",
        "    writer.add_scalars('GM', {'train': train_stats[4],\n",
        "                              'val': val_stats[4],\n",
        "                              }, epoch)\n",
        "    writer.add_scalars('WM', {'train': train_stats[5],\n",
        "                              'val': val_stats[5],\n",
        "                              }, epoch)\n",
        "    return\n",
        "\n",
        "\n",
        "def showgradients(model):\n",
        "    for param in model.parameters():\n",
        "        print(type(param.data), param.size())\n",
        "        print(\"GRADS= \\n\", param.grad)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def datestr():\n",
        "    now = time.gmtime()\n",
        "    return '{}{:02}{:02}_{:02}{:02}'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, path,  filename='last'):\n",
        "\n",
        "    name = os.path.join(path, filename+'_checkpoint.pth.tar')\n",
        "    print(name)\n",
        "    torch.save(state, name)\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model,optimizer, args, metrics, epoch, best_pred_loss,confusion_matrix):\n",
        "    loss = metrics.data['loss']\n",
        "    save_path = args.save\n",
        "    make_dirs(save_path)\n",
        "    \n",
        "    with open(save_path + '/training_arguments.txt', 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "    \n",
        "    is_best = False\n",
        "    if loss < best_pred_loss:\n",
        "        is_best = True\n",
        "        best_pred_loss = loss\n",
        "        save_checkpoint({'epoch': epoch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'metrics': metrics.data },\n",
        "                        is_best, save_path, args.model + \"_best\")\n",
        "        np.save(save_path + 'best_confusion_matrix.npy',confusion_matrix.cpu().numpy())\n",
        "            \n",
        "    else:\n",
        "        save_checkpoint({'epoch': epoch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'metrics': metrics.data},\n",
        "                        False, save_path, args.model + \"_last\")\n",
        "\n",
        "    return best_pred_loss\n",
        "\n",
        "\n",
        "def make_dirs(path):\n",
        "    if not os.path.exists(path):\n",
        "\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def create_stats_files(path):\n",
        "    train_f = open(os.path.join(path, 'train.csv'), 'w')\n",
        "    val_f = open(os.path.join(path, 'val.csv'), 'w')\n",
        "    return train_f, val_f\n",
        "\n",
        "\n",
        "def read_json_file(fname):\n",
        "    with open(fname, 'r') as handle:\n",
        "        return json.load(handle, object_hook=OrderedDict)\n",
        "\n",
        "\n",
        "def write_json_file(content, fname):\n",
        "    with open(fname, 'w') as handle:\n",
        "        json.dump(content, handle, indent=4, sort_keys=False)\n",
        "\n",
        "\n",
        "def read_filepaths(file):\n",
        "    paths, labels = [], []\n",
        "    with open(file, 'r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "        for idx, line in enumerate(lines):\n",
        "            if ('/ c o' in line):\n",
        "                break\n",
        "            subjid, path, label = line.split(' ')\n",
        "\n",
        "            paths.append(path)\n",
        "            labels.append(label)\n",
        "    return paths, labels\n",
        "\n",
        "\n",
        "\n",
        "def select_model(args):\n",
        "    if args.model == 'COVIDNet':\n",
        "        return CovidNet(args.classes)\n",
        "\n",
        "\n",
        "def select_optimizer(args, model):\n",
        "    if args.opt == 'sgd':\n",
        "        return optim.SGD(model.parameters(), lr=args.lr, momentum=0.5, weight_decay=args.weight_decay)\n",
        "    elif args.opt == 'adam':\n",
        "        return optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        return optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "\n",
        "def print_stats(args, epoch, num_samples, trainloader, metrics):\n",
        "    if (num_samples % args.log_interval == 1):\n",
        "        print(\"Epoch:{:2d}\\tSample:{:5d}/{:5d}\\tLoss:{:.4f}\\tAccuracy:{:.2f}\".format(epoch,\n",
        "                                                                                         num_samples,\n",
        "                                                                                         len(\n",
        "                                                                                             trainloader) * args.batch_size,\n",
        "                                                                                         metrics.data[\n",
        "                                                                                             'loss'] / num_samples,\n",
        "                                                                                         metrics.data[\n",
        "                                                                                             'correct'] /\n",
        "                                                                                         metrics.data[\n",
        "                                                                                             'total']))\n",
        "\n",
        "\n",
        "def print_summary(args, epoch, num_samples, metrics, mode=''):\n",
        "    print(mode + \"\\n SUMMARY EPOCH:{:2d}\\tSample:{:5d}/{:5d}\\tLoss:{:.4f}\\tAccuracy:{:.2f}\\n\".format(epoch,\n",
        "                                                                                                     num_samples,\n",
        "                                                                                                     num_samples ,\n",
        "                                                                                                     metrics.data[\n",
        "                                                                                                         'loss'] / num_samples,                                                                             \n",
        "                                                                                                     metrics.data[\n",
        "                                                                                                         'correct'] /\n",
        "                                                                                                     metrics.data[\n",
        "                                                                                                         'total']))\n",
        "\n",
        "\n",
        "def confusion_matrix(nb_classes):\n",
        "\n",
        "\n",
        "\n",
        "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            classes = classes.to(device)\n",
        "            outputs = model_ft(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    print(confusion_matrix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_fFFfn2qU2",
        "colab_type": "text"
      },
      "source": [
        "# METRICS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQath5FI2yVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self, path, keys=None, writer=None):\n",
        "        self.writer = writer\n",
        "\n",
        "        self.data = {'correct': 0,\n",
        "                     'total': 0,\n",
        "                     'loss': 0,\n",
        "                     'accuracy': 0,\n",
        "                     }\n",
        "        self.save_path = path\n",
        "\n",
        "    def reset(self):\n",
        "        for key in self.data:\n",
        "            self.data[key] = 0\n",
        "\n",
        "    def update_key(self, key, value, n=1):\n",
        "        if self.writer is not None:\n",
        "            self.writer.add_scalar(key, value)\n",
        "        self.data[key] += value\n",
        "\n",
        "    def update(self, values):\n",
        "        for key in self.data:\n",
        "            self.data[key] += values[key]\n",
        "\n",
        "    def avg_acc(self):\n",
        "        return self.data['correct'] / self.data['total']\n",
        "\n",
        "    def avg_loss(self):\n",
        "        return self.data['loss'] / self.data['total']\n",
        "\n",
        "    def save(self):\n",
        "        with open(self.save_path, 'w') as save_file:\n",
        "            a = 0  # csv.writer()\n",
        "            # TODO\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        assert pred.shape[0] == len(target)\n",
        "        correct = 0\n",
        "        correct += torch.sum(pred == target).item()\n",
        "    return correct, len(target), correct / len(target)\n",
        "\n",
        "\n",
        "def top_k_acc(output, target, k=3):\n",
        "    with torch.no_grad():\n",
        "        pred = torch.topk(output, k, dim=1)[1]\n",
        "        assert pred.shape[0] == len(target)\n",
        "        correct = 0\n",
        "        for i in range(k):\n",
        "            correct += torch.sum(pred[:, i] == target).item()\n",
        "    return correct / len(target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChF-utz429PK",
        "colab_type": "text"
      },
      "source": [
        "# LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3lCmUJ32-yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll_loss(output, target):\n",
        "    return F.nll_loss(output, target)\n",
        "\n",
        "\n",
        "def crossentropy_loss(output, target):\n",
        "    return F.cross_entropy(output, target)\n",
        "\n",
        "def focal_loss(output,target):\n",
        "    ce_loss = F.cross_entropy(output, target, reduction='none')\n",
        "    #print(ce_loss.shape)\n",
        "    pt = torch.exp(-ce_loss)\n",
        "    alpha = 0.25\n",
        "    gamma = 2\n",
        "    focal_loss = (alpha * (1-pt)**gamma * ce_loss).mean() \n",
        "    return focal_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiBN8qXfOoA",
        "colab_type": "text"
      },
      "source": [
        "# MODEL COVID-NET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeSXN3Th0Ba2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X99y49bfODF",
        "colab_type": "code",
        "outputId": "32087b3d-5432-414d-face-3b983011b6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class PEXP(nn.Module):\n",
        "    def __init__(self, n_input, n_out):\n",
        "        super(PEXP, self).__init__()\n",
        "\n",
        "        '''\n",
        "        • First-stage Projection: 1×1 convolutions for projecting input features to a lower dimension,\n",
        "\n",
        "        • Expansion: 1×1 convolutions for expanding features\n",
        "            to a higher dimension that is different than that of the\n",
        "            input features,\n",
        "\n",
        "\n",
        "        • Depth-wise Representation: efficient 3×3 depthwise convolutions for learning spatial characteristics to\n",
        "            minimize computational complexity while preserving\n",
        "            representational capacity,\n",
        "\n",
        "        • Second-stage Projection: 1×1 convolutions for projecting features back to a lower dimension, and\n",
        "\n",
        "        • Extension: 1×1 convolutions that finally extend channel dimensionality to a higher dimension to produce\n",
        "             the final features.\n",
        "             \n",
        "        # self.first_stage = nn.Conv2d(in_channels = n_input, out_channels=n_input//2, kernel_size=1)\n",
        "        # self.expansion = nn.Conv2d(in_channels = n_input//2, out_channels=int(3*n_input/4), kernel_size=1)\n",
        "        # self.dwc = nn.Conv2d(in_channels = int(3*n_input/4), out_channels=int(3*n_input/4), kernel_size=3,groups=int(3*n_input/4))\n",
        "        # self.second_stage = nn.Conv2d(in_channels = int(3*n_input/4), out_channels=n_input//2, kernel_size=1)\n",
        "        # self.expansion = nn.Conv2d(in_channels = n_input//2, out_channels=n_out, kernel_size=1)\n",
        "        self.network = nn.Sequential(nn.Conv2d(in_channels=n_input, out_channels=n_input // 2, kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 2, out_channels=int(3 * n_input / 4),\n",
        "                                               kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=int(3 * n_input / 4), out_channels=int(3 * n_input / 4),\n",
        "                                               kernel_size=3, groups=int(3 * n_input / 4), padding=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=int(3 * n_input / 4), out_channels=n_input // 2,\n",
        "                                               kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 2, out_channels=n_out, kernel_size=1))\n",
        "        '''\n",
        "\n",
        "\n",
        "        self.network = nn.Sequential(nn.Conv2d(in_channels=n_input, out_channels=n_input // 4, kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 4, out_channels=n_input // 2,\n",
        "                                               kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 2, out_channels=n_input // 2,\n",
        "                                               kernel_size=3, groups=n_input // 2, padding=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 2, out_channels=n_input // 4,\n",
        "                                               kernel_size=1),\n",
        "\n",
        "                                     nn.Conv2d(in_channels=n_input // 4, out_channels=n_out, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "\n",
        "class CovidNet(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super(CovidNet, self).__init__()\n",
        "        filters = {\n",
        "            'pexp1_1': [64, 256],\n",
        "            'pexp1_2': [256, 256],\n",
        "            'pexp1_3': [256, 256],\n",
        "            'pexp2_1': [256, 512],\n",
        "            'pexp2_2': [512, 512],\n",
        "            'pexp2_3': [512, 512],\n",
        "            'pexp2_4': [512, 512],\n",
        "            'pexp3_1': [512, 1024],\n",
        "            'pexp3_2': [1024, 1024],\n",
        "            'pexp3_3': [1024, 1024],\n",
        "            'pexp3_4': [1024, 1024],\n",
        "            'pexp3_5': [1024, 1024],\n",
        "            'pexp3_6': [1024, 1024],\n",
        "            'pexp4_1': [1024, 2048],\n",
        "            'pexp4_2': [2048, 2048],\n",
        "            'pexp4_3': [2048, 2048],\n",
        "        }\n",
        "\n",
        "        self.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3))\n",
        "        self.add_module('conv1_1x1', nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1))\n",
        "        self.add_module('conv2_1x1', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1))\n",
        "        self.add_module('conv3_1x1', nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1))\n",
        "        self.add_module('conv4_1x1', nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=1))\n",
        "        for key in filters:\n",
        "\n",
        "            if ('pool' in key):\n",
        "                self.add_module(key, nn.MaxPool2d(filters[key][0], filters[key][1]))\n",
        "            else:\n",
        "                self.add_module(key, PEXP(filters[key][0], filters[key][1]))\n",
        "        self.add_module('flatten', Flatten())\n",
        "        self.add_module('fc1', nn.Linear(7 * 7 * 2048, 1024))\n",
        "\n",
        "        self.add_module('fc2', nn.Linear(1024, 256))\n",
        "        self.add_module('classifier', nn.Linear(256, n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(self.conv1(x),2)\n",
        "        out_conv1_1x1 = self.conv1_1x1(x)\n",
        "\n",
        "        pepx11 = self.pexp1_1(x)\n",
        "        pepx12 = self.pexp1_2(pepx11 + out_conv1_1x1)\n",
        "        pepx13 = self.pexp1_3(pepx12 + pepx11 + out_conv1_1x1)\n",
        "\n",
        "        out_conv2_1x1 = F.max_pool2d(self.conv2_1x1(pepx12 + pepx11 + pepx13 +  out_conv1_1x1),2)\n",
        "\n",
        "        pepx21 = self.pexp2_1(F.max_pool2d(pepx13, 2) + F.max_pool2d(pepx11, 2) + F.max_pool2d(pepx12, 2) + F.max_pool2d(out_conv1_1x1,2))\n",
        "        pepx22 = self.pexp2_2(pepx21 + out_conv2_1x1)\n",
        "        pepx23 = self.pexp2_3(pepx22 + pepx21 + out_conv2_1x1)\n",
        "        pepx24 = self.pexp2_4(pepx23 + pepx21 + pepx22 + out_conv2_1x1)\n",
        "\n",
        "        out_conv3_1x1 = F.max_pool2d(self.conv3_1x1(pepx22 + pepx21 + pepx23 + pepx24 + out_conv2_1x1),2)\n",
        "\n",
        "        pepx31 = self.pexp3_1(F.max_pool2d(pepx24, 2) + F.max_pool2d(pepx21, 2) + F.max_pool2d(pepx22,2) + F.max_pool2d(pepx23, 2) + F.max_pool2d(out_conv2_1x1,2))\n",
        "        pepx32 = self.pexp3_2(pepx31 + out_conv3_1x1)\n",
        "        pepx33 = self.pexp3_3(pepx31 + pepx32)\n",
        "        pepx34 = self.pexp3_4(pepx31 + pepx32 + pepx33)\n",
        "        pepx35 = self.pexp3_5(pepx31 + pepx32 + pepx33 + pepx34)\n",
        "        pepx36 = self.pexp3_6(pepx31 + pepx32 + pepx33 + pepx34 + pepx35)\n",
        "\n",
        "        out_conv4_1x1 = F.max_pool2d(self.conv4_1x1(pepx31 + pepx32 + pepx33 + pepx34 + pepx35+ pepx36 + out_conv3_1x1),2)\n",
        "\n",
        "        pepx41 = self.pexp4_1(F.max_pool2d(pepx31, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx34, 2)+ F.max_pool2d(pepx35, 2)+ F.max_pool2d(pepx36, 2)+ F.max_pool2d(out_conv3_1x1,2))\n",
        "        pepx42 = self.pexp4_2(pepx41 + out_conv4_1x1)\n",
        "        pepx43 = self.pexp4_3(pepx41 + pepx42 + out_conv4_1x1)\n",
        "        flattened = self.flatten(pepx41 + pepx42 + pepx43 + out_conv4_1x1)\n",
        "\n",
        "        fc1out = F.relu(self.fc1(flattened))\n",
        "        fc2out = F.relu(self.fc2(fc1out))\n",
        "        logits = self.classifier(fc2out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        " FORWARD ONLY WITH SKIP CONNECTIONS\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.conv1(x))\n",
        "        out_conv1_1x1 = self.conv1_1x1(x)\n",
        "\n",
        "        pepx11 = self.pexp1_1(x)\n",
        "        pepx12 = self.pexp1_2(pepx11)\n",
        "        pepx13 = self.pexp1_3(pepx12 + pepx11)\n",
        "\n",
        "        pepx21 = self.pexp2_1(F.max_pool2d(pepx13, 2) + F.max_pool2d(pepx11, 2) + F.max_pool2d(pepx12, 2))\n",
        "        pepx22 = self.pexp2_2(pepx21)\n",
        "        pepx23 = self.pexp2_3(pepx22 + pepx21)\n",
        "        pepx24 = self.pexp2_4(pepx23 + pepx21 + pepx22)\n",
        "\n",
        "        pepx31 = self.pexp3_1(F.max_pool2d(pepx24, 2) + F.max_pool2d(pepx21, 2) + F.max_pool2d(pepx22,2) + F.max_pool2d(pepx23, 2))\n",
        "        pepx32 = self.pexp3_2(pepx31)\n",
        "        pepx33 = self.pexp3_3(pepx31 + pepx32)\n",
        "        pepx34 = self.pexp3_4(pepx31 + pepx32 + pepx33)\n",
        "        pepx35 = self.pexp3_5(pepx31 + pepx32 + pepx33 + pepx34)\n",
        "        pepx36 = self.pexp3_6(pepx31 + pepx32 + pepx33 + pepx34 + pepx35)\n",
        "\n",
        "        pepx41 = self.pexp4_1(F.max_pool2d(pepx31, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx34, 2)+ F.max_pool2d(pepx35, 2)+ F.max_pool2d(pepx36, 2))\n",
        "        pepx42 = self.pexp4_2(pepx41)\n",
        "        pepx43 = self.pexp4_3(pepx41 + pepx42)\n",
        "        flattened = self.flatten(pepx41 + pepx42 + pepx43)\n",
        "\n",
        "        fc1out = self.fc1(flattened)\n",
        "        fc2out = self.fc2(fc1out)\n",
        "        logits = self.classifier(fc2out)\n",
        "        return x\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n FORWARD ONLY WITH SKIP CONNECTIONS\\n\\n    def forward(self, x):\\n        x = self.pool1(self.conv1(x))\\n        out_conv1_1x1 = self.conv1_1x1(x)\\n\\n        pepx11 = self.pexp1_1(x)\\n        pepx12 = self.pexp1_2(pepx11)\\n        pepx13 = self.pexp1_3(pepx12 + pepx11)\\n\\n        pepx21 = self.pexp2_1(F.max_pool2d(pepx13, 2) + F.max_pool2d(pepx11, 2) + F.max_pool2d(pepx12, 2))\\n        pepx22 = self.pexp2_2(pepx21)\\n        pepx23 = self.pexp2_3(pepx22 + pepx21)\\n        pepx24 = self.pexp2_4(pepx23 + pepx21 + pepx22)\\n\\n        pepx31 = self.pexp3_1(F.max_pool2d(pepx24, 2) + F.max_pool2d(pepx21, 2) + F.max_pool2d(pepx22,2) + F.max_pool2d(pepx23, 2))\\n        pepx32 = self.pexp3_2(pepx31)\\n        pepx33 = self.pexp3_3(pepx31 + pepx32)\\n        pepx34 = self.pexp3_4(pepx31 + pepx32 + pepx33)\\n        pepx35 = self.pexp3_5(pepx31 + pepx32 + pepx33 + pepx34)\\n        pepx36 = self.pexp3_6(pepx31 + pepx32 + pepx33 + pepx34 + pepx35)\\n\\n        pepx41 = self.pexp4_1(F.max_pool2d(pepx31, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx32, 2) + F.max_pool2d(pepx34, 2)+ F.max_pool2d(pepx35, 2)+ F.max_pool2d(pepx36, 2))\\n        pepx42 = self.pexp4_2(pepx41)\\n        pepx43 = self.pexp4_3(pepx41 + pepx42)\\n        flattened = self.flatten(pepx41 + pepx42 + pepx43)\\n\\n        fc1out = self.fc1(flattened)\\n        fc2out = self.fc2(fc1out)\\n        logits = self.classifier(fc2out)\\n        return x\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYX88x25vF3h",
        "colab_type": "text"
      },
      "source": [
        "# DATALOADER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0g6lfu1-wtau",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class COVIDxDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Code for reading the COVIDxDataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode, n_classes=3, dataset_path='./datasets', dim=(224, 224)):\n",
        "        self.root = str(dataset_path)+'/'+mode+'/'\n",
        "       \n",
        "       \n",
        "        self.CLASSES = n_classes\n",
        "        self.dim = dim\n",
        "        self.COVIDxDICT = {'pneumonia': 0, 'normal': 1, 'COVID-19': 2}\n",
        "        testfile = '/content/test_split_v2.txt'\n",
        "        trainfile = '/content/train_split_v2.txt'\n",
        "        if (mode == 'train'):\n",
        "            self.paths, self.labels = read_filepaths(trainfile)\n",
        "        elif (mode == 'test'):\n",
        "            self.paths, self.labels = read_filepaths(testfile)\n",
        "        print(\"{} examples =  {}\".format(mode,len(self.paths)))\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_tensor = self.load_image(self.root+self.paths[index], self.dim, augmentation=self.mode)\n",
        "        label_tensor = torch.tensor(self.COVIDxDICT[self.labels[index]],dtype=torch.long)\n",
        "\n",
        "        return image_tensor,label_tensor\n",
        "\n",
        "    def load_image(self, img_path, dim, augmentation='test'):\n",
        "        if not os.path.exists(img_path):\n",
        "            print(\"IMAGE DOES NOT EXIST {}\".format(img_path))\n",
        "        image = Image.open(img_path).convert('RGB') \n",
        "        image = image.resize(dim).convert('RGB') \n",
        "        \n",
        "        #image.convert('RGB')\n",
        "        t = transforms.ToTensor()\n",
        "        # print(t(image).shape)\n",
        "        norm = transforms.Normalize(mean=[0.5, 0.5,0.5 ],\n",
        "                                    std=[1, 1, 1])\n",
        "\n",
        "        image_tensor = norm(t(image))\n",
        "       \n",
        "        # if(image_tensor.size(0)>1):\n",
        "        #     #print(img_path,\" > 1 channels\")\n",
        "        #     image_tensor = image_tensor.mean(dim=0,keepdim=True)\n",
        "        return image_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItoCpmm4vf0f",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRkbalc4vhsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def initialize(args):\n",
        "    if args.device is not None:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n",
        "    model = select_model(args)\n",
        "    \n",
        "    optimizer = select_optimizer(args,model)\n",
        "    if (args.cuda):\n",
        "        model.cuda()\n",
        "\n",
        "    train_params = {'batch_size': args.batch_size,\n",
        "                    'shuffle': True,\n",
        "                    'num_workers': 2}\n",
        "\n",
        "    test_params = {'batch_size': args.batch_size,\n",
        "                   'shuffle': False,\n",
        "                   'num_workers': 1}\n",
        "\n",
        "    train_loader = COVIDxDataset(mode='train', n_classes=args.classes, dataset_path=args.dataset,\n",
        "                                 dim=(224, 224))\n",
        "    val_loader = COVIDxDataset(mode='test', n_classes=args.classes, dataset_path=args.dataset,\n",
        "                               dim=(224, 224))\n",
        "    training_generator = DataLoader(train_loader, **train_params)\n",
        "    val_generator = DataLoader(val_loader, **test_params)\n",
        "    return model, optimizer,training_generator,val_generator\n",
        "\n",
        "\n",
        "def train(args, model, trainloader, optimizer, epoch):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    metrics = Metrics('')\n",
        "    metrics.reset()\n",
        "    for batch_idx, input_tensors in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_data, target = input_tensors\n",
        "        if (args.cuda):\n",
        "            input_data = input_data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        output = model(input_data)\n",
        "\n",
        "        loss = focal_loss(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        correct, total, acc = accuracy(output, target)\n",
        "\n",
        "        num_samples = batch_idx * args.batch_size + 1\n",
        "        metrics.update({'correct': correct, 'total': total, 'loss': loss.item(), 'accuracy': acc})\n",
        "        print_stats(args, epoch, num_samples, trainloader, metrics)\n",
        "\n",
        "    print_summary(args, epoch, num_samples, metrics, mode=\"Training\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def validation(args, model, testloader, epoch):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss(size_average='mean')\n",
        "\n",
        "    metrics = Metrics('')\n",
        "    metrics.reset()\n",
        "    confusion_matrix = torch.zeros(args.classes, args.classes)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, input_tensors in enumerate(testloader):\n",
        "\n",
        "            input_data, target = input_tensors\n",
        "            if (args.cuda):\n",
        "                input_data = input_data.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            output = model(input_data)\n",
        "\n",
        "            loss = focal_loss(output, target)\n",
        "\n",
        "            correct, total, acc = accuracy(output, target)\n",
        "            num_samples = batch_idx * args.batch_size + 1\n",
        "            _, preds = torch.max(output, 1)\n",
        "            for t, p in zip(target.cpu().view(-1), preds.cpu().view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "            metrics.update({'correct': correct, 'total': total, 'loss': loss.item(), 'accuracy': acc})\n",
        "            #print_stats(args, epoch, num_samples, testloader, metrics)\n",
        "\n",
        "    print_summary(args, epoch, num_samples, metrics, mode=\"Validation\")\n",
        "    return metrics,confusion_matrix\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz8BF21Lvl6a",
        "colab_type": "text"
      },
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9BF-bre0Y_M",
        "colab_type": "code",
        "outputId": "d5055ec7-68e9-4d59-cd33-d1c80e41442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    args = get_arguments()\n",
        "    SEED = args.seed\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(SEED)\n",
        "    if(args.cuda):\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "    model, optimizer,training_generator,val_generator = initialize(args)\n",
        "    \n",
        "    print(model)\n",
        "\n",
        "    best_pred_loss = 1000.0\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, min_lr=1e-5, verbose=True)\n",
        "    print('Checkpoint folder ',args.save)\n",
        "    #writer = SummaryWriter(log_dir='../runs/' + args.model, comment=args.model)\n",
        "    for epoch in range(1, args.nEpochs + 1):\n",
        "        train(args, model, training_generator, optimizer, epoch)\n",
        "        val_metrics,confusion_matrix = validation(args, model, val_generator, epoch)\n",
        "        #confusion_matrix = torch.tensor([0.0])\n",
        "        #val_metrics = Metrics('')\n",
        "        best_pred_loss = save_model(model,optimizer, args,val_metrics, epoch, best_pred_loss,confusion_matrix)\n",
        "        #print('avg lpss ' ,val_metrics.avg_loss())\n",
        "        print(confusion_matrix)\n",
        "        scheduler.step(val_metrics.avg_loss())\n",
        "        \n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch_size', type=int, default=4)\n",
        "    parser.add_argument('--log_interval', type=int, default=1000)\n",
        "    parser.add_argument('--dataset_name', type=str, default=\"COVIDx\")\n",
        "    parser.add_argument('--nEpochs', type=int, default=250)\n",
        "    parser.add_argument('--device', type=int, default=0)\n",
        "    parser.add_argument('--seed', type=int, default=123)\n",
        "    parser.add_argument('--classes', type=int, default=3)\n",
        "    parser.add_argument('--inChannels', type=int, default=1)\n",
        "    parser.add_argument('--lr', default=2e-5, type=float,\n",
        "                        help='learning rate (default: 1e-3)')\n",
        "    parser.add_argument('--weight_decay', default=1e-7, type=float,\n",
        "                        help='weight decay (default: 1e-6)')\n",
        "    parser.add_argument('--cuda', action='store_true', default=True)\n",
        "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
        "                        help='path to latest checkpoint (default: none)')\n",
        "    parser.add_argument('--model', type=str, default='COVIDNet',\n",
        "                        choices=('COVIDNET'))\n",
        "    parser.add_argument('--opt', type=str, default='adam',\n",
        "                        choices=('sgd', 'adam', 'rmsprop'))\n",
        "    parser.add_argument('--dataset', type=str, default='/content/covid-chestxray-dataset/data',\n",
        "                        help='path to dataset ')\n",
        "    parser.add_argument('--save', type=str, default='/content/drive/My Drive/MEDICAL/saved/COVIDNet'+datestr() ,\n",
        "                        help='path to checkpoint ')\n",
        "    args = parser.parse_args([])\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train examples =  16576\n",
            "test examples =  1953\n",
            "CovidNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "  (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv2_1x1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv3_1x1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv4_1x1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (pexp1_1): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
            "      (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp1_2): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
            "      (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp1_3): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
            "      (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp2_1): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
            "      (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp2_2): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
            "      (3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp2_3): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
            "      (3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp2_4): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
            "      (3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_1): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
            "      (3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_2): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_3): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_4): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_5): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp3_6): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp4_1): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
            "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp4_2): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
            "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pexp4_3): PEXP(\n",
            "    (network): Sequential(\n",
            "      (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
            "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (4): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (flatten): Flatten()\n",
            "  (fc1): Linear(in_features=100352, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n",
            "Checkpoint folder  /content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638\n",
            "Epoch: 1\tSample:    1/16576\tLoss:0.1188\tAccuracy:0.00\n",
            "Epoch: 1\tSample: 1001/16576\tLoss:0.0116\tAccuracy:0.67\n",
            "Epoch: 1\tSample: 2001/16576\tLoss:0.0106\tAccuracy:0.75\n",
            "Epoch: 1\tSample: 3001/16576\tLoss:0.0096\tAccuracy:0.77\n",
            "Epoch: 1\tSample: 4001/16576\tLoss:0.0092\tAccuracy:0.78\n",
            "Epoch: 1\tSample: 5001/16576\tLoss:0.0089\tAccuracy:0.79\n",
            "Epoch: 1\tSample: 6001/16576\tLoss:0.0087\tAccuracy:0.80\n",
            "Epoch: 1\tSample: 7001/16576\tLoss:0.0087\tAccuracy:0.80\n",
            "Epoch: 1\tSample: 8001/16576\tLoss:0.0085\tAccuracy:0.81\n",
            "Epoch: 1\tSample: 9001/16576\tLoss:0.0084\tAccuracy:0.81\n",
            "Epoch: 1\tSample:10001/16576\tLoss:0.0083\tAccuracy:0.82\n",
            "Epoch: 1\tSample:11001/16576\tLoss:0.0081\tAccuracy:0.82\n",
            "Epoch: 1\tSample:12001/16576\tLoss:0.0080\tAccuracy:0.82\n",
            "Epoch: 1\tSample:13001/16576\tLoss:0.0080\tAccuracy:0.82\n",
            "Epoch: 1\tSample:14001/16576\tLoss:0.0078\tAccuracy:0.83\n",
            "Epoch: 1\tSample:15001/16576\tLoss:0.0078\tAccuracy:0.83\n",
            "Epoch: 1\tSample:16001/16576\tLoss:0.0077\tAccuracy:0.83\n",
            "Training\n",
            " SUMMARY EPOCH: 1\tSample:16573/16573\tLoss:0.0076\tAccuracy:0.83\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation\n",
            " SUMMARY EPOCH: 1\tSample: 1953/ 1953\tLoss:0.0063\tAccuracy:0.87\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_best_checkpoint.pth.tar\n",
            "tensor([[952., 106.,   0.],\n",
            "        [144., 741.,   0.],\n",
            "        [  7.,   3.,   0.]])\n",
            "Epoch: 2\tSample:    1/16576\tLoss:0.0036\tAccuracy:1.00\n",
            "Epoch: 2\tSample: 1001/16576\tLoss:0.0063\tAccuracy:0.88\n",
            "Epoch: 2\tSample: 2001/16576\tLoss:0.0063\tAccuracy:0.88\n",
            "Epoch: 2\tSample: 3001/16576\tLoss:0.0061\tAccuracy:0.88\n",
            "Epoch: 2\tSample: 4001/16576\tLoss:0.0059\tAccuracy:0.88\n",
            "Epoch: 2\tSample: 5001/16576\tLoss:0.0059\tAccuracy:0.87\n",
            "Epoch: 2\tSample: 6001/16576\tLoss:0.0059\tAccuracy:0.87\n",
            "Epoch: 2\tSample: 7001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample: 8001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample: 9001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample:10001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample:11001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample:12001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample:13001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Epoch: 2\tSample:14001/16576\tLoss:0.0060\tAccuracy:0.87\n",
            "Epoch: 2\tSample:15001/16576\tLoss:0.0060\tAccuracy:0.87\n",
            "Epoch: 2\tSample:16001/16576\tLoss:0.0061\tAccuracy:0.87\n",
            "Training\n",
            " SUMMARY EPOCH: 2\tSample:16573/16573\tLoss:0.0060\tAccuracy:0.87\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 2\tSample: 1953/ 1953\tLoss:0.0056\tAccuracy:0.89\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_best_checkpoint.pth.tar\n",
            "tensor([[948., 110.,   0.],\n",
            "        [ 89., 796.,   0.],\n",
            "        [  6.,   4.,   0.]])\n",
            "Epoch: 3\tSample:    1/16576\tLoss:0.0133\tAccuracy:0.75\n",
            "Epoch: 3\tSample: 1001/16576\tLoss:0.0055\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 2001/16576\tLoss:0.0055\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 3001/16576\tLoss:0.0052\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 4001/16576\tLoss:0.0052\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 5001/16576\tLoss:0.0052\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 6001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 7001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 8001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample: 9001/16576\tLoss:0.0054\tAccuracy:0.89\n",
            "Epoch: 3\tSample:10001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:11001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:12001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:13001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:14001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:15001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Epoch: 3\tSample:16001/16576\tLoss:0.0053\tAccuracy:0.89\n",
            "Training\n",
            " SUMMARY EPOCH: 3\tSample:16573/16573\tLoss:0.0053\tAccuracy:0.89\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 3\tSample: 1953/ 1953\tLoss:0.0068\tAccuracy:0.88\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_last_checkpoint.pth.tar\n",
            "tensor([[875., 183.,   0.],\n",
            "        [ 44., 841.,   0.],\n",
            "        [  4.,   6.,   0.]])\n",
            "Epoch: 4\tSample:    1/16576\tLoss:0.0048\tAccuracy:1.00\n",
            "Epoch: 4\tSample: 1001/16576\tLoss:0.0048\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 2001/16576\tLoss:0.0050\tAccuracy:0.89\n",
            "Epoch: 4\tSample: 3001/16576\tLoss:0.0046\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 4001/16576\tLoss:0.0047\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 5001/16576\tLoss:0.0046\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 6001/16576\tLoss:0.0047\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 7001/16576\tLoss:0.0048\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 8001/16576\tLoss:0.0048\tAccuracy:0.90\n",
            "Epoch: 4\tSample: 9001/16576\tLoss:0.0048\tAccuracy:0.90\n",
            "Epoch: 4\tSample:10001/16576\tLoss:0.0047\tAccuracy:0.90\n",
            "Epoch: 4\tSample:11001/16576\tLoss:0.0047\tAccuracy:0.90\n",
            "Epoch: 4\tSample:12001/16576\tLoss:0.0046\tAccuracy:0.90\n",
            "Epoch: 4\tSample:13001/16576\tLoss:0.0046\tAccuracy:0.90\n",
            "Epoch: 4\tSample:14001/16576\tLoss:0.0046\tAccuracy:0.90\n",
            "Epoch: 4\tSample:15001/16576\tLoss:0.0045\tAccuracy:0.90\n",
            "Epoch: 4\tSample:16001/16576\tLoss:0.0045\tAccuracy:0.90\n",
            "Training\n",
            " SUMMARY EPOCH: 4\tSample:16573/16573\tLoss:0.0045\tAccuracy:0.90\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 4\tSample: 1953/ 1953\tLoss:0.0055\tAccuracy:0.89\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_best_checkpoint.pth.tar\n",
            "tensor([[977.,  81.,   0.],\n",
            "        [126., 759.,   0.],\n",
            "        [  5.,   5.,   0.]])\n",
            "Epoch: 5\tSample:    1/16576\tLoss:0.0996\tAccuracy:0.50\n",
            "Epoch: 5\tSample: 1001/16576\tLoss:0.0034\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 2001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 3001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 4001/16576\tLoss:0.0037\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 5001/16576\tLoss:0.0037\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 6001/16576\tLoss:0.0038\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 7001/16576\tLoss:0.0038\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 8001/16576\tLoss:0.0037\tAccuracy:0.92\n",
            "Epoch: 5\tSample: 9001/16576\tLoss:0.0039\tAccuracy:0.91\n",
            "Epoch: 5\tSample:10001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Epoch: 5\tSample:11001/16576\tLoss:0.0038\tAccuracy:0.92\n",
            "Epoch: 5\tSample:12001/16576\tLoss:0.0038\tAccuracy:0.92\n",
            "Epoch: 5\tSample:13001/16576\tLoss:0.0038\tAccuracy:0.92\n",
            "Epoch: 5\tSample:14001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Epoch: 5\tSample:15001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Epoch: 5\tSample:16001/16576\tLoss:0.0039\tAccuracy:0.92\n",
            "Training\n",
            " SUMMARY EPOCH: 5\tSample:16573/16573\tLoss:0.0039\tAccuracy:0.92\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 5\tSample: 1953/ 1953\tLoss:0.0053\tAccuracy:0.90\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_best_checkpoint.pth.tar\n",
            "tensor([[982.,  75.,   1.],\n",
            "        [114., 771.,   0.],\n",
            "        [  6.,   4.,   0.]])\n",
            "Epoch: 6\tSample:    1/16576\tLoss:0.0148\tAccuracy:1.00\n",
            "Epoch: 6\tSample: 1001/16576\tLoss:0.0031\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 2001/16576\tLoss:0.0031\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 3001/16576\tLoss:0.0031\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 4001/16576\tLoss:0.0032\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 5001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 6001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 7001/16576\tLoss:0.0032\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 8001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample: 9001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:10001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:11001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:12001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:13001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:14001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:15001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Epoch: 6\tSample:16001/16576\tLoss:0.0033\tAccuracy:0.93\n",
            "Training\n",
            " SUMMARY EPOCH: 6\tSample:16573/16573\tLoss:0.0033\tAccuracy:0.93\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 6\tSample: 1953/ 1953\tLoss:0.0057\tAccuracy:0.89\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_last_checkpoint.pth.tar\n",
            "tensor([[995.,  63.,   0.],\n",
            "        [148., 736.,   1.],\n",
            "        [  6.,   4.,   0.]])\n",
            "Epoch: 7\tSample:    1/16576\tLoss:0.0090\tAccuracy:1.00\n",
            "Epoch: 7\tSample: 1001/16576\tLoss:0.0022\tAccuracy:0.95\n",
            "Epoch: 7\tSample: 2001/16576\tLoss:0.0023\tAccuracy:0.95\n",
            "Epoch: 7\tSample: 3001/16576\tLoss:0.0022\tAccuracy:0.95\n",
            "Epoch: 7\tSample: 4001/16576\tLoss:0.0024\tAccuracy:0.95\n",
            "Epoch: 7\tSample: 5001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample: 6001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample: 7001/16576\tLoss:0.0024\tAccuracy:0.94\n",
            "Epoch: 7\tSample: 8001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample: 9001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample:10001/16576\tLoss:0.0026\tAccuracy:0.94\n",
            "Epoch: 7\tSample:11001/16576\tLoss:0.0026\tAccuracy:0.94\n",
            "Epoch: 7\tSample:12001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample:13001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample:14001/16576\tLoss:0.0025\tAccuracy:0.94\n",
            "Epoch: 7\tSample:15001/16576\tLoss:0.0026\tAccuracy:0.94\n",
            "Epoch: 7\tSample:16001/16576\tLoss:0.0026\tAccuracy:0.94\n",
            "Training\n",
            " SUMMARY EPOCH: 7\tSample:16573/16573\tLoss:0.0026\tAccuracy:0.94\n",
            "\n",
            "Validation\n",
            " SUMMARY EPOCH: 7\tSample: 1953/ 1953\tLoss:0.0070\tAccuracy:0.89\n",
            "\n",
            "/content/drive/My Drive/MEDICAL/saved/COVIDNet20200401_0638/COVIDNet_last_checkpoint.pth.tar\n",
            "tensor([[897., 161.,   0.],\n",
            "        [ 40., 845.,   0.],\n",
            "        [  4.,   6.,   0.]])\n",
            "Epoch: 8\tSample:    1/16576\tLoss:0.0035\tAccuracy:1.00\n",
            "Epoch: 8\tSample: 1001/16576\tLoss:0.0017\tAccuracy:0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}